{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc898a64-7f5e-4dd4-8c36-8a84c9c23cc6",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization\n",
    "* This notebook is used to optimize hyperparameters\n",
    "* all results are saved in 'Thesis/HPO'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8273ab8-bf63-445c-b106-3cf2e0069cf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd07edfa-1d08-45b5-8dfc-b1e139a14099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/lurny/files/Thesis\n"
     ]
    }
   ],
   "source": [
    "#setting rood directory into ../NeuralPowerDisaggregation\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1c25207-6a37-4b41-8a97-0fe4a877110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.configs import *\n",
    "config1 = get_config_for_dataset('NOAA_Weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed67d911-f7f1-4738-8bbd-1c5c20c7d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-23--08-36-39601029\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, ParameterSampler, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "#from pydl85 import DL85Classifier\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if config1['computation']['use_gpu']:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(config1['computation']['gpu_numbers'])\n",
    "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "    os.environ['XLA_FLAGS'] = \"--xla_gpu_cuda_data_dir=/usr/local/cuda-11.6\"\n",
    "    os.environ['TF_XLA_FLAGS'] = \"--tf_xla_enable_xla_devices --tf_xla_auto_jit=2\"    \n",
    "else:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'false' \n",
    "#os.environ['TF_XLA_FLAGS'] = \"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit\" \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from utilities.utilities_GDT import *\n",
    "from utilities.GDT_for_streams import *\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from itertools import product\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "from skmultiflow.trees import HoeffdingTreeClassifier\n",
    "from skmultiflow.trees import HoeffdingAdaptiveTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from livelossplot import PlotLosses\n",
    "import time\n",
    "\n",
    "tf.random.set_seed(config1['computation']['random_seed'])\n",
    "np.random.seed(config1['computation']['random_seed'])\n",
    "random.seed(config1['computation']['random_seed'])\n",
    "\n",
    "from datetime import datetime\n",
    "timestr = datetime.utcnow().strftime('%Y-%m-%d--%H-%M-%S%f')\n",
    "print(timestr)\n",
    "os.makedirs(os.path.dirname(\"./evaluation_results/latex_tables/\" + timestr +\"/\"), exist_ok=True)\n",
    "\n",
    "filepath = './evaluation_results/depth' + str(config1['gdt']['depth']) + '/' + timestr + '/'\n",
    "Path(filepath).mkdir(parents=True, exist_ok=True)    \n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6027aa6-692d-468d-a6a4-93b4f414a1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num XLA-GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71fa12-9914-4f47-a2e5-68a8751c8ce3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2eec461-b6f2-488d-9069-a548d8f91a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape (selected):  (10000, 30)\n",
      "Original Data Shape (encoded):  (10000, 30)\n",
      "Original Data Class Distribution:  4137  (true) / 5863  (false)\n",
      "(6000, 30) (6000,)\n",
      "(2000, 30) (2000,)\n",
      "(2000, 30) (2000,)\n",
      "Min Ratio:  0.41783333333333333\n",
      "Min Ratio:  0.41475\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 76\u001b[0m\n\u001b[1;32m     52\u001b[0m     ((X_train, y_train),\n\u001b[1;32m     53\u001b[0m      (X_valid, y_valid),\n\u001b[1;32m     54\u001b[0m      (X_test, y_test),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m                                        random_seed\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m,\u001b[38;5;66;03m#random_seed,\u001b[39;00m\n\u001b[1;32m     63\u001b[0m                                        verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#verbosity)  \u001b[39;00m\n\u001b[1;32m     64\u001b[0m     dataset_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     65\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m'\u001b[39m: X_train,\n\u001b[1;32m     66\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m'\u001b[39m: y_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalizer_list\u001b[39m\u001b[38;5;124m'\u001b[39m: normalizer_list\n\u001b[1;32m     72\u001b[0m            }\n\u001b[0;32m---> 76\u001b[0m     \u001b[43mmodel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGDT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m              \u001b[49m\u001b[43mdataset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \u001b[43m              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_training\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgdt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_training\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgdt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpretrain_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[43m              \u001b[49m\u001b[43mrestarts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#config_test['gdt']['restarts'], \u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;66;43;03m#restart_type=config_test['gdt']['restart_type'], \u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[1;32m     85\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;66;43;03m#early_stopping_epochs=config_training['gdt']['early_stopping_epochs'], \u001b[39;49;00m\n\u001b[1;32m     86\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;66;43;03m#early_stopping_type=config_test['gdt']['early_stopping_type'],\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_valid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_valid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m#Start Stream\u001b[39;00m\n\u001b[1;32m     96\u001b[0m X_data_stream \u001b[38;5;241m=\u001b[39m split_df_chunks(X_data, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomputation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/work/lurny/files/Thesis/utilities/GDT_for_streams.py:367\u001b[0m, in \u001b[0;36mGDT.partial_fit\u001b[0;34m(self, X_train, y_train, batch_size, epochs, restarts, restart_type, early_stopping_epochs, early_stopping_type, early_stopping_epsilon, valid_data, drift_flag)\u001b[0m\n\u001b[1;32m    365\u001b[0m         path_identifier \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mfloormod(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mfloor(leaf_index\u001b[38;5;241m/\u001b[39m(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;241m-\u001b[39mcurrent_depth)))), \u001b[38;5;241m2\u001b[39m), tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    366\u001b[0m         internal_node_index \u001b[38;5;241m=\u001b[39m  tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m, (current_depth\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), tf\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mfloor(leaf_index\u001b[38;5;241m/\u001b[39m(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;241m-\u001b[39m(current_depth\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))))), tf\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m, tf\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m--> 367\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_identifier_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_identifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minternal_node_index_list\u001b[38;5;241m.\u001b[39mappend(internal_node_index)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_identifier_list \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(tf\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_identifier_list), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/trackable/data_structures.py:635\u001b[0m, in \u001b[0;36mListWrapper.append\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m\"\"\"Add a new trackable value.\"\"\"\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_external_modification()\n\u001b[0;32m--> 635\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mListWrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_snapshot()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/trackable/data_structures.py:399\u001b[0m, in \u001b[0;36mList.append\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    398\u001b[0m   \u001b[38;5;124;03m\"\"\"Add a new trackable value.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_track_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name_element\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage\u001b[38;5;241m.\u001b[39mappend(value)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/trackable/data_structures.py:706\u001b[0m, in \u001b[0;36mListWrapper._track_value\u001b[0;34m(self, value, name)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03m\"\"\"Allows storage of non-trackable objects.\"\"\"\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mListWrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_track_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    708\u001b[0m   \u001b[38;5;66;03m# Even if this value isn't trackable, we need to make sure\u001b[39;00m\n\u001b[1;32m    709\u001b[0m   \u001b[38;5;66;03m# NoDependency objects get unwrapped.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m   value \u001b[38;5;241m=\u001b[39m sticky_attribute_assignment(\n\u001b[1;32m    711\u001b[0m       trackable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, value\u001b[38;5;241m=\u001b[39mvalue, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/trackable/data_structures.py:206\u001b[0m, in \u001b[0;36mTrackableDataStructure._track_value\u001b[0;34m(self, value, name)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_track_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, value, name):\n\u001b[1;32m    205\u001b[0m   \u001b[38;5;124;03m\"\"\"Add a dependency on `value`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[43msticky_attribute_assignment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrackable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, variables\u001b[38;5;241m.\u001b[39mVariable):\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_extra_variables\u001b[38;5;241m.\u001b[39mappend(value)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/trackable/data_structures.py:132\u001b[0m, in \u001b[0;36msticky_attribute_assignment\u001b[0;34m(trackable, name, value)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m    129\u001b[0m   \u001b[38;5;66;03m# pylint: enable=unidiomatic-typecheck\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.tracking.sticky_attribute_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msticky_attribute_assignment\u001b[39m(trackable, name, value):\n\u001b[1;32m    134\u001b[0m   \u001b[38;5;124;03m\"\"\"Adds dependencies, generally called from __setattr__.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m  This behavior is shared between Trackable and Model.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    NoDependency object if necessary).\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    151\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, NoDependency):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#'NOAA_Weather','agr_a','agr_g','airlines','electricity','hyperplane','rbf_f','rbf_m','sea_a','sea_g'\n",
    "dataset_names = ['airlines']\n",
    "depth_array = [12,13,14]\n",
    "for dataset_name in dataset_names:\n",
    "    config = get_config_for_dataset(dataset_name)\n",
    "    for depth in depth_array:\n",
    "        config['gdt']['depth'] = depth\n",
    "        config['computation']['gpu_numbers'] = '2'\n",
    "\n",
    "        from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "        config_training, metrics = prepare_training_for_streams(identifier = 'BIN:'+dataset_name, config = config)\n",
    "        #load Dataset\n",
    "        X_data, y_data, nominal_features, ordinal_features = load_dataset_for_streams(identifier = 'BIN:'+dataset_name, \n",
    "                                                                                      max_total_samples = config['computation']['max_total_samples'])\n",
    "\n",
    "        model_dict = {}\n",
    "        scores_dict = {}\n",
    "        normalizer_list = []\n",
    "        plotlosses_benchmark = PlotLosses()\n",
    "        verbosity = 1\n",
    "        model_dict['GDT'] = GDT(number_of_variables = len(X_data.columns),\n",
    "                    number_of_classes = len(np.unique(y_data)),#dataset_dict['number_of_classes'],\n",
    "\n",
    "                    objective = config_training['gdt']['objective'],\n",
    "\n",
    "                    depth = config_training['gdt']['depth'],\n",
    "\n",
    "                    learning_rate_index = config_training['gdt']['learning_rate_index'],\n",
    "                    learning_rate_values = config_training['gdt']['learning_rate_values'],\n",
    "                    learning_rate_leaf = config_training['gdt']['learning_rate_leaf'],\n",
    "\n",
    "                    optimizer = config_training['gdt']['optimizer'],\n",
    "\n",
    "                    loss = 'crossentropy',\n",
    "\n",
    "                    initializer_values = config_training['gdt']['initializer_values'],\n",
    "                    initializer_index = config_training['gdt']['initializer_index'],\n",
    "                    initializer_leaf = config_training['gdt']['initializer_leaf'],        \n",
    "\n",
    "                    random_seed = config_training['computation']['random_seed'],\n",
    "                    verbosity = verbosity)  \n",
    "\n",
    "\n",
    "        #Pretraing\n",
    "        if(config_training['computation']['pretrain_size']>0 and len(X_data) > config_training['computation']['pretrain_size']):\n",
    "            X_pretrain_data = X_data.iloc[:config_training['computation']['pretrain_size'],:]\n",
    "            X_data = X_data.iloc[config_training['computation']['pretrain_size']:,:]\n",
    "            y_pretrain_data = y_data.iloc[:config_training['computation']['pretrain_size']]\n",
    "            y_data = y_data.iloc[config_training['computation']['pretrain_size']:]\n",
    "\n",
    "            ((X_train, y_train),\n",
    "             (X_valid, y_valid),\n",
    "             (X_test, y_test),\n",
    "             (X_train_with_valid, y_train_with_valid),\n",
    "             normalizer_list) = preprocess_data(X_pretrain_data, \n",
    "                                               y_pretrain_data,\n",
    "                                               nominal_features,\n",
    "                                               ordinal_features,\n",
    "                                               config_training,\n",
    "                                               normalizer_list,\n",
    "                                               random_seed= 42,#random_seed,\n",
    "                                               verbosity=1)#verbosity)  \n",
    "            dataset_dict = {\n",
    "                   'X_train': X_train,\n",
    "                   'y_train': y_train,\n",
    "                   'X_valid': X_valid,\n",
    "                   'y_valid': y_valid,\n",
    "                   'X_test': X_test,\n",
    "                   'y_test': y_test,\n",
    "                   'normalizer_list': normalizer_list\n",
    "                   }\n",
    "\n",
    "\n",
    "\n",
    "            model_dict['GDT'].partial_fit(dataset_dict['X_train'],\n",
    "                      dataset_dict['y_train'],\n",
    "\n",
    "                      batch_size=config_training['gdt']['batch_size'], \n",
    "                      epochs=config_training['gdt']['pretrain_epochs'], \n",
    "\n",
    "                      restarts = 0,#config_test['gdt']['restarts'], \n",
    "                      #restart_type=config_test['gdt']['restart_type'], \n",
    "\n",
    "                      #early_stopping_epochs=config_training['gdt']['early_stopping_epochs'], \n",
    "                      #early_stopping_type=config_test['gdt']['early_stopping_type'],\n",
    "\n",
    "                      valid_data=(dataset_dict['X_valid'],dataset_dict['y_valid']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Start Stream\n",
    "        X_data_stream = split_df_chunks(X_data, config['computation']['chunk_size'])\n",
    "        y_data_stream = split_df_chunks(y_data, config['computation']['chunk_size'])\n",
    "\n",
    "        scores_GDT = {'train':np.array([]),\n",
    "                      'valid':np.array([]),\n",
    "                      'test':np.array([])\n",
    "                     }\n",
    "        scores_VFDT = {'train':np.array([]),\n",
    "                  'test':np.array([])\n",
    "                 }\n",
    "        scores_CVFDT = {'train':np.array([]),\n",
    "                      'test':np.array([])\n",
    "                     }\n",
    "\n",
    "        timer = {'training':{\n",
    "                    'GDT':0.0,\n",
    "                    'VFDT':0.0,\n",
    "                    'CVFDT':0.0,\n",
    "                 },\n",
    "                 'prediction':{\n",
    "                    'GDT':0.0,\n",
    "                    'VFDT':0.0,\n",
    "                    'CVFDT':0.0,\n",
    "                 }\n",
    "                }\n",
    "        flag = False;\n",
    "\n",
    "        for i in range(len(X_data_stream)):\n",
    "            normalizer_list=[]\n",
    "            t0 = time.time()\n",
    "            ((X_train, y_train),\n",
    "             (X_valid, y_valid),\n",
    "             (X_test, y_test),\n",
    "             (X_train_with_valid, y_train_with_valid),\n",
    "             normalizer_list) = preprocess_data(X_data_stream[i], \n",
    "                                               y_data_stream[i],\n",
    "                                               nominal_features,\n",
    "                                               ordinal_features,\n",
    "                                               config_training,\n",
    "                                               normalizer_list,\n",
    "                                               random_seed= 42,#random_seed,\n",
    "                                               verbosity=1)#verbosity)  \n",
    "            dataset_dict = {\n",
    "                   'X_train': X_train,\n",
    "                   'y_train': y_train,\n",
    "                   'X_valid': X_valid,\n",
    "                   'y_valid': y_valid,\n",
    "                   'X_test': X_test,\n",
    "                   'y_test': y_test,\n",
    "                   'normalizer_list': normalizer_list\n",
    "                   } \n",
    "\n",
    "            t1 = time.time()\n",
    "\n",
    "            t0 = time.time()\n",
    "            history = model_dict['GDT'].partial_fit(dataset_dict['X_train'],\n",
    "                      dataset_dict['y_train'],\n",
    "\n",
    "                      batch_size=config_training['gdt']['batch_size'], \n",
    "                      epochs=config_training['gdt']['epochs'], \n",
    "\n",
    "                      restarts = 0,\n",
    "                      drift_flag = flag,\n",
    "                      #config_test['gdt']['restarts'], \n",
    "                      #restart_type=config_test['gdt']['restart_type'], \n",
    "\n",
    "                      #early_stopping_epochs=config_training['gdt']['early_stopping_epochs'], \n",
    "                      #early_stopping_type=config_test['gdt']['early_stopping_type'],\n",
    "\n",
    "                      valid_data=(dataset_dict['X_valid'],dataset_dict['y_valid']))\n",
    "\n",
    "            flag=False\n",
    "            t1 = time.time()\n",
    "            timer['training']['GDT'] = timer['training']['GDT']+t1-t0\n",
    "\n",
    "\n",
    "            ###EVALUATION\n",
    "            y_test_data =dataset_dict['y_test'].values\n",
    "            temp_X_test = dataset_dict['X_test'].values\n",
    "\n",
    "\n",
    "            #GDT\n",
    "            metric = \"f1\"\n",
    "           # y_test_data = dataset_dict['y_test']\n",
    "            y_pred_GDT_train = model_dict['GDT'].predict(enforce_numpy(dataset_dict['X_train'].values))\n",
    "            y_pred_GDT_train = np.nan_to_num(y_pred_GDT_train)\n",
    "            y_pred_GDT_train = np.round(y_pred_GDT_train)\n",
    "\n",
    "            y_pred_GDT_valid = model_dict['GDT'].predict(enforce_numpy(dataset_dict['X_valid'].values))\n",
    "            y_pred_GDT_valid = np.nan_to_num(y_pred_GDT_valid)\n",
    "            y_pred_GDT_valid = np.round(y_pred_GDT_valid)    \n",
    "\n",
    "            y_pred_GDT_test = model_dict['GDT'].predict(enforce_numpy(temp_X_test))\n",
    "            y_pred_GDT_test = np.nan_to_num(y_pred_GDT_test)\n",
    "            y_pred_GDT_test = np.round(y_pred_GDT_test)\n",
    "\n",
    "\n",
    "            GDT_f1_train = f1_score(dataset_dict['y_train'], y_pred_GDT_train)\n",
    "            GDT_f1_valid = f1_score(dataset_dict['y_valid'], y_pred_GDT_valid)\n",
    "            GDT_f1_test = f1_score(dataset_dict['y_test'], y_pred_GDT_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            scores_GDT['train'] = np.append(scores_GDT['train'], GDT_f1_train)\n",
    "            scores_GDT['valid'] = np.append(scores_GDT['valid'], GDT_f1_valid)\n",
    "            scores_GDT['test'] = np.append(scores_GDT['test'], GDT_f1_test)\n",
    "\n",
    "\n",
    "\n",
    "        results = pd.DataFrame(columns=['metric','train','validation','test'])\n",
    "        results.loc[len(results)] = ['f1',round(np.average(scores_GDT['train']),4),round(np.average(scores_GDT['valid']),4),round(np.average(scores_GDT['test']),4)]\n",
    "        results.to_csv('HPO/depth'+str(depth)+'/'+dataset_name+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
