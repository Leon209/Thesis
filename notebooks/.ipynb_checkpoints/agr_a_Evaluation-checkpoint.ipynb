{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc898a64-7f5e-4dd4-8c36-8a84c9c23cc6",
   "metadata": {},
   "source": [
    "<h1>Imports<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd07edfa-1d08-45b5-8dfc-b1e139a14099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/lurny/files/Thesis\n"
     ]
    }
   ],
   "source": [
    "#setting rood directory into ../NeuralPowerDisaggregation\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23e79b7a-f7db-4692-b44c-f9f4bef9e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'gdt': {\n",
    "        'depth': 11,\n",
    "                \n",
    "        'learning_rate_index': 0.05,\n",
    "        'learning_rate_values': 0.01,\n",
    "        'learning_rate_leaf': 0.005,\n",
    "        \n",
    "        'dropout': 0, #0.2 oder 0.5\n",
    "        \n",
    "        \n",
    "        'initializer_values': 'GlorotUniform', \n",
    "        'initializer_index': 'GlorotUniform', \n",
    "        'initializer_leaf': 'GlorotUniform', \n",
    "        \n",
    "        'optimizer': 'adam', \n",
    "        \n",
    "        'batch_size': 500,#120\n",
    "        'epochs': 1,\n",
    "        \n",
    "        'restarts': 0,#\n",
    "        'restart_type': 'loss', #'loss', 'metric'\n",
    "        \n",
    "        'early_stopping_epochs': 600,\n",
    "        'early_stopping_type': 'loss', #'loss', 'metric'\n",
    "        'early_stopping_epsilon': 0.0,\n",
    "        \n",
    "        'pretrain_epochs': 20,\n",
    "    },\n",
    "    \n",
    "    'preprocessing': {\n",
    "        'balance_threshold': 0,#.25, #if minclass fraction less than threshold/num_classes | #0=no rebalance, 1=rebalance all\n",
    "        'normalization_technique': 'mean', #'min-max'\n",
    "    },\n",
    "\n",
    "    'computation': {\n",
    "        'random_seed': 42,\n",
    "        'trials': 10, # fixed to 1 for HPO\n",
    "        \n",
    "        'use_best_hpo_result': True,\n",
    "        'force_depth': False,\n",
    "        \n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '3',#'1',\n",
    "        'n_jobs': 10, #vorher 20\n",
    "        'verbosity': 0,\n",
    "        \n",
    "        \n",
    "        'hpo': None,#'binary', #'binary', 'multi', 'regression'\n",
    "        'search_iterations': 300,\n",
    "        'cv_num': 3,     \n",
    "        \n",
    "        'metrics_class': ['f1', 'roc_auc', 'accuracy'],\n",
    "        \n",
    "        'metrics_reg': ['r2', 'neg_mean_absolute_percentage_error', 'neg_mean_absolute_error', 'neg_mean_squared_error'],\n",
    "        \n",
    "        'eval_metric_class': ['f1', 'roc_auc'], #f1 accuracy\n",
    "        'eval_metric_reg': 'r2', #r2 mae        \n",
    "        \n",
    "        'max_total_samples': 1000000,\n",
    "        'chunk_size': 20000,#default 200\n",
    "        'pretrain_size': 0,\n",
    "    },\n",
    "    \n",
    "    'benchmarks': {\n",
    "        #'sklearn': True,\n",
    "        #'GeneticTree': True,        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed67d911-f7f1-4738-8bbd-1c5c20c7d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-07--13-50-45141945\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, ParameterSampler, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "#from pydl85 import DL85Classifier\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if config['computation']['use_gpu']:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(config['computation']['gpu_numbers'])\n",
    "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "    os.environ['XLA_FLAGS'] = \"--xla_gpu_cuda_data_dir=/usr/local/cuda-11.6\"\n",
    "    os.environ['TF_XLA_FLAGS'] = \"--tf_xla_enable_xla_devices --tf_xla_auto_jit=2\"    \n",
    "else:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'false' \n",
    "#os.environ['TF_XLA_FLAGS'] = \"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit\" \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from utilities.utilities_GDT import *\n",
    "from utilities.GDT_for_streams import *\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from itertools import product\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "tf.random.set_seed(config['computation']['random_seed'])\n",
    "np.random.seed(config['computation']['random_seed'])\n",
    "random.seed(config['computation']['random_seed'])\n",
    "\n",
    "from datetime import datetime\n",
    "timestr = datetime.utcnow().strftime('%Y-%m-%d--%H-%M-%S%f')\n",
    "print(timestr)\n",
    "os.makedirs(os.path.dirname(\"./evaluation_results/latex_tables/\" + timestr +\"/\"), exist_ok=True)\n",
    "\n",
    "filepath = './evaluation_results/depth' + str(config['gdt']['depth']) + '/' + timestr + '/'\n",
    "Path(filepath).mkdir(parents=True, exist_ok=True)    \n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6027aa6-692d-468d-a6a4-93b4f414a1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num XLA-GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6664e044-346a-432c-b863-c757829efa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def split_df_chunks(data_df,chunk_size):\n",
    "    total_length     = len(data_df)\n",
    "    normal_chunk_num = math.floor(total_length/chunk_size)\n",
    "    chunks = []\n",
    "    for i in range(normal_chunk_num):\n",
    "        chunk = data_df[(i*chunk_size):((i+1)*chunk_size)]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71fa12-9914-4f47-a2e5-68a8751c8ce3",
   "metadata": {},
   "source": [
    "# Evaluation of agr_a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eec461-b6f2-488d-9069-a548d8f91a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape (selected):  (10000, 36)\n",
      "Original Data Shape (encoded):  (10000, 36)\n",
      "Original Data Class Distribution:  3246  (true) / 6754  (false)\n",
      "(6000, 36) (6000,)\n",
      "(2000, 36) (2000,)\n",
      "(2000, 36) (2000,)\n",
      "Min Ratio:  0.322\n"
     ]
    }
   ],
   "source": [
    "from skmultiflow.trees import HoeffdingTreeClassifier\n",
    "from skmultiflow.trees import HoeffdingAdaptiveTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from livelossplot import PlotLosses\n",
    "from skmultiflow.drift_detection.adwin import ADWIN\n",
    "import time\n",
    "\n",
    "\n",
    "VFDT_classifier = HoeffdingTreeClassifier()\n",
    "CVFDT_classifier = HoeffdingAdaptiveTreeClassifier(split_confidence = 0.001)\n",
    "adwin = ADWIN()\n",
    "\n",
    "config_training, metrics = prepare_training_for_streams(identifier = 'BIN:agr_a', config = config)\n",
    "#load Dataset\n",
    "X_data, y_data, nominal_features, ordinal_features = load_dataset_for_streams(identifier = 'BIN:agr_a', \n",
    "                                                                              max_total_samples = config['computation']['max_total_samples'])\n",
    "\n",
    "\n",
    "\n",
    "model_dict = {}\n",
    "scores_dict = {}\n",
    "normalizer_list = []\n",
    "plotlosses_benchmark = PlotLosses()\n",
    "verbosity = 1\n",
    "model_dict['GDT'] = GDT(number_of_variables = len(X_data.columns),\n",
    "            number_of_classes = len(np.unique(y_data)),#dataset_dict['number_of_classes'],\n",
    "\n",
    "            objective = config_training['gdt']['objective'],\n",
    "\n",
    "            depth = config_training['gdt']['depth'],\n",
    "\n",
    "            learning_rate_index = config_training['gdt']['learning_rate_index'],\n",
    "            learning_rate_values = config_training['gdt']['learning_rate_values'],\n",
    "            learning_rate_leaf = config_training['gdt']['learning_rate_leaf'],\n",
    "\n",
    "            optimizer = config_training['gdt']['optimizer'],\n",
    "\n",
    "            loss = 'crossentropy',\n",
    "\n",
    "            initializer_values = config_training['gdt']['initializer_values'],\n",
    "            initializer_index = config_training['gdt']['initializer_index'],\n",
    "            initializer_leaf = config_training['gdt']['initializer_leaf'],        \n",
    "\n",
    "            random_seed = config_training['computation']['random_seed'],\n",
    "            verbosity = verbosity)  \n",
    "\n",
    "\n",
    "#Pretraing\n",
    "if(config_training['computation']['pretrain_size']>0 and len(X_data) > config_training['computation']['pretrain_size']):\n",
    "    X_pretrain_data = X_data.iloc[:config_training['computation']['pretrain_size'],:]\n",
    "    X_data = X_data.iloc[config_training['computation']['pretrain_size']:,:]\n",
    "    y_pretrain_data = y_data.iloc[:config_training['computation']['pretrain_size']]\n",
    "    y_data = y_data.iloc[config_training['computation']['pretrain_size']:]\n",
    "    \n",
    "    ((X_train, y_train),\n",
    "     (X_valid, y_valid),\n",
    "     (X_test, y_test),\n",
    "     normalizer_list) = preprocess_data(X_pretrain_data, \n",
    "                                       y_pretrain_data,\n",
    "                                       nominal_features,\n",
    "                                       ordinal_features,\n",
    "                                       config_training,\n",
    "                                       normalizer_list,\n",
    "                                       random_seed= 42,#random_seed,\n",
    "                                       verbosity=1)#verbosity)  \n",
    "    dataset_dict = {\n",
    "           'X_train': X_train,\n",
    "           'y_train': y_train,\n",
    "           'X_valid': X_valid,\n",
    "           'y_valid': y_valid,\n",
    "           'X_test': X_test,\n",
    "           'y_test': y_test,\n",
    "           'normalizer_list': normalizer_list\n",
    "           }\n",
    "    \n",
    "    \n",
    "    \n",
    "    model_dict['GDT'].partial_fit(dataset_dict['X_train'],\n",
    "              dataset_dict['y_train'],\n",
    "\n",
    "              batch_size=config_training['gdt']['batch_size'], \n",
    "              epochs=config_training['gdt']['pretrain_epochs'], \n",
    "\n",
    "              restarts = 0,#config_test['gdt']['restarts'], \n",
    "              #restart_type=config_test['gdt']['restart_type'], \n",
    "\n",
    "              #early_stopping_epochs=config_training['gdt']['early_stopping_epochs'], \n",
    "              #early_stopping_type=config_test['gdt']['early_stopping_type'],\n",
    "\n",
    "              valid_data=(dataset_dict['X_valid'],dataset_dict['y_valid']))\n",
    "    \n",
    "    #pretrain benchmarks\n",
    "    temp_X_train =dataset_dict['X_train'].values\n",
    "    temp_y_train =dataset_dict['y_train'].values\n",
    "    for i in range(0, len(dataset_dict['X_train'])):\n",
    "        VFDT_classifier.partial_fit(np.array([temp_X_train[i]], np.float64), np.array([temp_y_train[i]], np.float64)) \n",
    "        CVFDT_classifier.partial_fit(np.array([temp_X_train[i]], np.float64), np.array([temp_y_train[i]], np.float64))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Start Stream\n",
    "X_data_stream = split_df_chunks(X_data, config['computation']['chunk_size'])\n",
    "y_data_stream = split_df_chunks(y_data, config['computation']['chunk_size'])\n",
    "\n",
    "preprocessing_time = 0\n",
    "GDT_time = 0\n",
    "VFDT_time = 0\n",
    "training_time = 0\n",
    "CVFDT_time = 0\n",
    "scores_GDT = {'f1':np.array([]),\n",
    "              'acc':np.array([])\n",
    "             }\n",
    "scores_VFDT = {'f1':np.array([]),\n",
    "              'acc':np.array([])\n",
    "             }\n",
    "scores_CVFDT = {'f1':np.array([]),\n",
    "              'acc':np.array([])\n",
    "             }\n",
    "flag = False;\n",
    "\n",
    "for i in range(len(X_data_stream)):\n",
    "    t0 = time.time()\n",
    "    ((X_train, y_train),\n",
    "     (X_valid, y_valid),\n",
    "     (X_test, y_test),\n",
    "     normalizer_list) = preprocess_data(X_data_stream[i], \n",
    "                                       y_data_stream[i],\n",
    "                                       nominal_features,\n",
    "                                       ordinal_features,\n",
    "                                       config_training,\n",
    "                                       normalizer_list,\n",
    "                                       random_seed= 42,#random_seed,\n",
    "                                       verbosity=1)#verbosity)  \n",
    "    dataset_dict = {\n",
    "           'X_train': X_train,\n",
    "           'y_train': y_train,\n",
    "           'X_valid': X_valid,\n",
    "           'y_valid': y_valid,\n",
    "           'X_test': X_test,\n",
    "           'y_test': y_test,\n",
    "           'normalizer_list': normalizer_list\n",
    "           }\n",
    "    \n",
    "    t1 = time.time()\n",
    "    preprocessing_time = preprocessing_time + t1-t0\n",
    "\n",
    "    t0 = time.time()\n",
    "    if(i==25):\n",
    "        flag = True\n",
    "    history = model_dict['GDT'].partial_fit(dataset_dict['X_train'],\n",
    "              dataset_dict['y_train'],\n",
    "\n",
    "              batch_size=config_training['gdt']['batch_size'], \n",
    "              epochs=config_training['gdt']['epochs'], \n",
    "\n",
    "              restarts = 0,\n",
    "              drift_flag = flag,\n",
    "              #config_test['gdt']['restarts'], \n",
    "              #restart_type=config_test['gdt']['restart_type'], \n",
    "\n",
    "              #early_stopping_epochs=config_training['gdt']['early_stopping_epochs'], \n",
    "              #early_stopping_type=config_test['gdt']['early_stopping_type'],\n",
    "\n",
    "              valid_data=(dataset_dict['X_valid'],dataset_dict['y_valid']))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    GDT_time = GDT_time + t1-t0\n",
    "    training_time = training_time + t1-t0\n",
    "    \n",
    "    ###EVALUATION\n",
    "    y_test_data =dataset_dict['y_test'].tolist()\n",
    "    \n",
    "    temp_X_train =dataset_dict['X_train'].values\n",
    "    temp_y_train =dataset_dict['y_train'].values\n",
    "    temp_X_test = dataset_dict['X_test'].values\n",
    "    \n",
    "    t0 = time.time()\n",
    "    #GDT\n",
    "    metric = \"f1\"\n",
    "    y_test_data = dataset_dict['y_test']\n",
    "    y_pred_GDT = model_dict['GDT'].predict(enforce_numpy(temp_X_test))\n",
    "    y_pred_GDT = np.nan_to_num(y_pred_GDT)\n",
    "    y_pred_GDT = np.round(y_pred_GDT)\n",
    "    #GDT_f1 = sklearn.metrics.get_scorer(metric)._score_func(y_pred_GDT, y_test, average='weighted')\n",
    "    GDT_f1 = accuracy_score(y_test_data, y_pred_GDT)\n",
    "    GDT_acc = accuracy_score(y_test_data, y_pred_GDT)\n",
    "       \n",
    "    t1 = time.time()\n",
    "    GDT_time = GDT_time + t1-t0\n",
    "    \n",
    "\n",
    "    #VFDT\n",
    "    t0 = time.time()\n",
    "    y_pred_VFDT = []\n",
    "    for i in range(0, len(dataset_dict['X_train'])):\n",
    "        VFDT_classifier.partial_fit(np.array([temp_X_train[i]], np.float64), np.array([temp_y_train[i]], np.float64))\n",
    "        adwin.add_element((X_train,y_train))\n",
    "    for i in range(0, len(dataset_dict['X_test'])):\n",
    "        y_pred_VFDT.append(VFDT_classifier.predict(np.array([temp_X_test[i]], np.float64))[0])  \n",
    "    VFDT_f1 = f1_score(y_test_data, y_pred_VFDT)\n",
    "    VFDT_acc = accuracy_score(y_test_data, y_pred_VFDT)\n",
    "    t1 = time.time()\n",
    "    VFDT_time = VFDT_time + t1-t0\n",
    "    \n",
    "    #CVFDT\n",
    "    t0 = time.time()\n",
    "    y_pred_CVFDT = []\n",
    "    for i in range(0, len(dataset_dict['X_train'])):\n",
    "        CVFDT_classifier.partial_fit(np.array([temp_X_train[i]], np.float64), np.array([temp_y_train[i]], np.float64))\n",
    "    for i in range(0, len(dataset_dict['X_test'])):\n",
    "        y_pred_CVFDT.append(CVFDT_classifier.predict(np.array([temp_X_test[i]], np.float64))[0])  \n",
    "    CVFDT_f1 = f1_score(y_test_data, y_pred_CVFDT)\n",
    "    CVFDT_acc = accuracy_score(y_test_data, y_pred_CVFDT)\n",
    "    t1 = time.time()\n",
    "    CVFDT_time = VFDT_time + t1-t0\n",
    "    \n",
    "    \n",
    "    plotlosses_benchmark.update({'GDT_f1': GDT_f1,'VFDT_f1': VFDT_f1, 'CVFDT_f1':CVFDT_f1})\n",
    "    plotlosses_benchmark.send() \n",
    "    \n",
    "    \n",
    "    scores_GDT['f1'] = np.append(scores_GDT['f1'], GDT_f1)\n",
    "    scores_VFDT['f1'] = np.append(scores_VFDT['f1'], VFDT_f1)\n",
    "    scores_CVFDT['f1'] = np.append(scores_CVFDT['f1'], CVFDT_f1)\n",
    "    scores_GDT['acc'] = np.append(scores_GDT['acc'], GDT_acc)\n",
    "    scores_VFDT['acc'] = np.append(scores_VFDT['acc'], VFDT_acc)\n",
    "    scores_CVFDT['acc'] = np.append(scores_CVFDT['acc'], CVFDT_acc)\n",
    "    \n",
    "    \n",
    "     \n",
    "\n",
    "    #scores_dict['GDT'] = calculate_scores(model_dict = model_dict, \n",
    "     #                              dataset_dict = dataset_dict, \n",
    "      #                             scores_dict = prepare_score_dict(config=config_training), \n",
    "       #                            metrics = metrics)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f3407-c608-413e-8235-aefca11c4d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot results\n",
    "X = np.arange(0, len(scores_GDT['f1']), 1)\n",
    "  \n",
    "# Assign variables to the y axis part of the curve\n",
    "  \n",
    "# Plotting both the curves simultaneously\n",
    "plt.plot(X, scores_GDT['f1'], color='r', label='GDT_f1', linewidth=1)\n",
    "plt.plot(X, scores_VFDT['f1'], color='g', label='VFDT_f1', linewidth=1)\n",
    "plt.plot(X, scores_CVFDT['f1'], color='y', label='CVFDT_f1', linewidth=1)\n",
    "  \n",
    "# Naming the x-axis, y-axis and the whole graph\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.title(\"Dataset\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"median GDT F1: \" + str(np.median(scores_GDT['f1'])))\n",
    "print(\"median VFDT F1: \" + str(np.median(scores_VFDT['f1'])))\n",
    "print(\"median CVFDT F1: \" + str(np.median(scores_CVFDT['f1'])))\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"preprocessing_time: \" + str(preprocessing_time))\n",
    "print(\"training_time: \"+ str(training_time))\n",
    "print(\"GDT_time: \" + str(GDT_time))\n",
    "print(\"VFDT_time: \" + str(VFDT_time))\n",
    "print(\"CVFDT_time: \" + str(CVFDT_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c1cf8-2639-4196-b29d-5fc32dcb503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot results\n",
    "X = np.arange(0, len(scores_GDT['acc']), 1)\n",
    "  \n",
    "# Assign variables to the y axis part of the curve\n",
    "  \n",
    "# Plotting both the curves simultaneously\n",
    "plt.plot(X, scores_GDT['acc'], color='r', label='GDT Acc', linewidth=1)\n",
    "plt.plot(X, scores_VFDT['acc'], color='g', label='VFDT Acc', linewidth=1)\n",
    "plt.plot(X, scores_CVFDT['acc'], color='y', label='CVFDT Acc', linewidth=1)\n",
    "  \n",
    "# Naming the x-axis, y-axis and the whole graph\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Dataset\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"median GDT Accuracy: \" + str(np.median(scores_GDT['acc'])))\n",
    "print(\"median VFDT Accuracy: \" + str(np.median(scores_VFDT['acc'])))\n",
    "print(\"median CVFDT Accuracy: \" + str(np.median(scores_CVFDT['acc'])))\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"preprocessing_time: \" + str(preprocessing_time))\n",
    "print(\"training_time: \"+ str(training_time))\n",
    "print(\"GDT_time: \" + str(GDT_time))\n",
    "print(\"VFDT_time: \" + str(VFDT_time))\n",
    "print(\"CVFDT_time: \" + str(CVFDT_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c1bbfc8-ee4d-4522-8982-5a4e342e4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets_streaming/airlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3103e133-3bcd-4c8e-b6ac-525e84a0b315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    299119\n",
       "1    240264\n",
       "Name: Delay, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Delay.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a35523-c76f-4258-8637-acf8b79091ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
