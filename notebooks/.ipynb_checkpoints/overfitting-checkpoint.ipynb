{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc898a64-7f5e-4dd4-8c36-8a84c9c23cc6",
   "metadata": {},
   "source": [
    "<h1>Imports<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd07edfa-1d08-45b5-8dfc-b1e139a14099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/lurny/files/Thesis\n"
     ]
    }
   ],
   "source": [
    "#setting rood directory into ../NeuralPowerDisaggregation\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1c25207-6a37-4b41-8a97-0fe4a877110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.configs import *\n",
    "config1 = get_config_for_dataset('NOAA_Weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed67d911-f7f1-4738-8bbd-1c5c20c7d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-16--11-57-14805359\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, ParameterSampler, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "#from pydl85 import DL85Classifier\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if config1['computation']['use_gpu']:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(config1['computation']['gpu_numbers'])\n",
    "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "    os.environ['XLA_FLAGS'] = \"--xla_gpu_cuda_data_dir=/usr/local/cuda-11.6\"\n",
    "    os.environ['TF_XLA_FLAGS'] = \"--tf_xla_enable_xla_devices --tf_xla_auto_jit=2\"    \n",
    "else:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'false' \n",
    "#os.environ['TF_XLA_FLAGS'] = \"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit\" \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from utilities.utilities_GDT import *\n",
    "from utilities.GDT_for_streams import *\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from itertools import product\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "from skmultiflow.trees import HoeffdingTreeClassifier\n",
    "from skmultiflow.trees import HoeffdingAdaptiveTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from livelossplot import PlotLosses\n",
    "import time\n",
    "\n",
    "tf.random.set_seed(config1['computation']['random_seed'])\n",
    "np.random.seed(config1['computation']['random_seed'])\n",
    "random.seed(config1['computation']['random_seed'])\n",
    "\n",
    "from datetime import datetime\n",
    "timestr = datetime.utcnow().strftime('%Y-%m-%d--%H-%M-%S%f')\n",
    "print(timestr)\n",
    "os.makedirs(os.path.dirname(\"./evaluation_results/latex_tables/\" + timestr +\"/\"), exist_ok=True)\n",
    "\n",
    "filepath = './evaluation_results/depth' + str(config1['gdt']['depth']) + '/' + timestr + '/'\n",
    "Path(filepath).mkdir(parents=True, exist_ok=True)    \n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6027aa6-692d-468d-a6a4-93b4f414a1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num XLA-GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6664e044-346a-432c-b863-c757829efa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def split_df_chunks(data_df,chunk_size):\n",
    "    total_length     = len(data_df)\n",
    "    normal_chunk_num = math.floor(total_length/chunk_size)\n",
    "    chunks = []\n",
    "    for i in range(normal_chunk_num):\n",
    "        chunk = data_df[(i*chunk_size):((i+1)*chunk_size)]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71fa12-9914-4f47-a2e5-68a8751c8ce3",
   "metadata": {},
   "source": [
    "# Evaluation of agr_a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eec461-b6f2-488d-9069-a548d8f91a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape (selected):  (2500, 10)\n",
      "Original Data Shape (encoded):  (2500, 10)\n",
      "Original Data Class Distribution:  1167  (true) / 1333  (false)\n",
      "(1500, 10) (1500,)\n",
      "(500, 10) (500,)\n",
      "(500, 10) (500,)\n",
      "Min Ratio:  0.4746666666666667\n",
      "Min Ratio:  0.478\n"
     ]
    }
   ],
   "source": [
    "#'NOAA_Weather','agr_a','agr_g','airlines','electricity','hyperplane','rbf_f',\n",
    "dataset_names = ['rbf_m','sea_a','sea_g']\n",
    "for dataset_name in dataset_names:\n",
    "    VFDT_classifier = HoeffdingTreeClassifier()\n",
    "    CVFDT_classifier = HoeffdingAdaptiveTreeClassifier(split_confidence = 0.001)\n",
    "    config = get_config_for_dataset(dataset_name)\n",
    "\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "    config_training, metrics = prepare_training_for_streams(identifier = 'BIN:'+dataset_name, config = config)\n",
    "    #load Dataset\n",
    "    X_data, y_data, nominal_features, ordinal_features = load_dataset_for_streams(identifier = 'BIN:'+dataset_name, \n",
    "                                                                                  max_total_samples = config['computation']['max_total_samples'])\n",
    "\n",
    "    model_dict = {}\n",
    "    scores_dict = {}\n",
    "    normalizer_list = []\n",
    "    plotlosses_benchmark = PlotLosses()\n",
    "    verbosity = 1\n",
    "    model_dict['GDT'] = GDT(number_of_variables = len(X_data.columns),\n",
    "                number_of_classes = len(np.unique(y_data)),#dataset_dict['number_of_classes'],\n",
    "\n",
    "                objective = config_training['gdt']['objective'],\n",
    "\n",
    "                depth = config_training['gdt']['depth'],\n",
    "\n",
    "                learning_rate_index = config_training['gdt']['learning_rate_index'],\n",
    "                learning_rate_values = config_training['gdt']['learning_rate_values'],\n",
    "                learning_rate_leaf = config_training['gdt']['learning_rate_leaf'],\n",
    "\n",
    "                optimizer = config_training['gdt']['optimizer'],\n",
    "\n",
    "                loss = 'crossentropy',\n",
    "\n",
    "                initializer_values = config_training['gdt']['initializer_values'],\n",
    "                initializer_index = config_training['gdt']['initializer_index'],\n",
    "                initializer_leaf = config_training['gdt']['initializer_leaf'],        \n",
    "\n",
    "                random_seed = config_training['computation']['random_seed'],\n",
    "                verbosity = verbosity)  \n",
    "\n",
    "\n",
    "    #Pretraing\n",
    "    if(config_training['computation']['pretrain_size']>0 and len(X_data) > config_training['computation']['pretrain_size']):\n",
    "        X_pretrain_data = X_data.iloc[:config_training['computation']['pretrain_size'],:]\n",
    "        X_data = X_data.iloc[config_training['computation']['pretrain_size']:,:]\n",
    "        y_pretrain_data = y_data.iloc[:config_training['computation']['pretrain_size']]\n",
    "        y_data = y_data.iloc[config_training['computation']['pretrain_size']:]\n",
    "\n",
    "        ((X_train, y_train),\n",
    "         (X_valid, y_valid),\n",
    "         (X_test, y_test),\n",
    "         (X_train_with_valid, y_train_with_valid),\n",
    "         normalizer_list) = preprocess_data(X_pretrain_data, \n",
    "                                           y_pretrain_data,\n",
    "                                           nominal_features,\n",
    "                                           ordinal_features,\n",
    "                                           config_training,\n",
    "                                           normalizer_list,\n",
    "                                           random_seed= 42,#random_seed,\n",
    "                                           verbosity=1)#verbosity)  \n",
    "        dataset_dict = {\n",
    "               'X_train': X_train,\n",
    "               'y_train': y_train,\n",
    "               'X_valid': X_valid,\n",
    "               'y_valid': y_valid,\n",
    "               'X_test': X_test,\n",
    "               'y_test': y_test,\n",
    "               'normalizer_list': normalizer_list\n",
    "               }\n",
    "\n",
    "\n",
    "\n",
    "        model_dict['GDT'].partial_fit(dataset_dict['X_train'],\n",
    "                  dataset_dict['y_train'],\n",
    "\n",
    "                  batch_size=config_training['gdt']['batch_size'], \n",
    "                  epochs=config_training['gdt']['pretrain_epochs'], \n",
    "\n",
    "                  restarts = 0,#config_test['gdt']['restarts'], \n",
    "                  #restart_type=config_test['gdt']['restart_type'], \n",
    "\n",
    "                  #early_stopping_epochs=config_training['gdt']['early_stopping_epochs'], \n",
    "                  #early_stopping_type=config_test['gdt']['early_stopping_type'],\n",
    "\n",
    "                  valid_data=(dataset_dict['X_valid'],dataset_dict['y_valid']))\n",
    "        \n",
    "        #pretrain benchmarks\n",
    "        #(X_train_with_valid, y_train_with_valid)\n",
    "        temp_X_train =X_train_with_valid.values\n",
    "        temp_y_train =y_train_with_valid.values\n",
    "        for i in range(0, len(dataset_dict['X_train'])):\n",
    "            VFDT_classifier.partial_fit(np.array([temp_X_train[i]], np.float64), np.array([temp_y_train[i]], np.float64)) \n",
    "            CVFDT_classifier.partial_fit(np.array([temp_X_train[i]], np.float64), np.array([temp_y_train[i]], np.float64))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Start Stream\n",
    "    X_data_stream = split_df_chunks(X_data, config['computation']['chunk_size'])\n",
    "    y_data_stream = split_df_chunks(y_data, config['computation']['chunk_size'])\n",
    "\n",
    "    scores_GDT = {'train':np.array([]),\n",
    "                  'valid':np.array([]),\n",
    "                  'test':np.array([])\n",
    "                 }\n",
    "    scores_VFDT = {'train':np.array([]),\n",
    "              'test':np.array([])\n",
    "             }\n",
    "    scores_CVFDT = {'train':np.array([]),\n",
    "                  'test':np.array([])\n",
    "                 }\n",
    "    \n",
    "    timer = {'training':{\n",
    "                'GDT':0.0,\n",
    "                'VFDT':0.0,\n",
    "                'CVFDT':0.0,\n",
    "             },\n",
    "             'prediction':{\n",
    "                'GDT':0.0,\n",
    "                'VFDT':0.0,\n",
    "                'CVFDT':0.0,\n",
    "             }\n",
    "            }\n",
    "    flag = False;\n",
    "\n",
    "    for i in range(len(X_data_stream)):\n",
    "        normalizer_list=[]\n",
    "        t0 = time.time()\n",
    "        ((X_train, y_train),\n",
    "         (X_valid, y_valid),\n",
    "         (X_test, y_test),\n",
    "         (X_train_with_valid, y_train_with_valid),\n",
    "         normalizer_list) = preprocess_data(X_data_stream[i], \n",
    "                                           y_data_stream[i],\n",
    "                                           nominal_features,\n",
    "                                           ordinal_features,\n",
    "                                           config_training,\n",
    "                                           normalizer_list,\n",
    "                                           random_seed= 42,#random_seed,\n",
    "                                           verbosity=1)#verbosity)  \n",
    "        dataset_dict = {\n",
    "               'X_train': X_train,\n",
    "               'y_train': y_train,\n",
    "               'X_valid': X_valid,\n",
    "               'y_valid': y_valid,\n",
    "               'X_test': X_test,\n",
    "               'y_test': y_test,\n",
    "               'normalizer_list': normalizer_list\n",
    "               } \n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        t0 = time.time()\n",
    "        history = model_dict['GDT'].partial_fit(dataset_dict['X_train'],\n",
    "                  dataset_dict['y_train'],\n",
    "\n",
    "                  batch_size=config_training['gdt']['batch_size'], \n",
    "                  epochs=config_training['gdt']['epochs'], \n",
    "\n",
    "                  restarts = 0,\n",
    "                  drift_flag = flag,\n",
    "                  #config_test['gdt']['restarts'], \n",
    "                  #restart_type=config_test['gdt']['restart_type'], \n",
    "\n",
    "                  #early_stopping_epochs=config_training['gdt']['early_stopping_epochs'], \n",
    "                  #early_stopping_type=config_test['gdt']['early_stopping_type'],\n",
    "\n",
    "                  valid_data=(dataset_dict['X_valid'],dataset_dict['y_valid']))\n",
    "\n",
    "        flag=False\n",
    "        t1 = time.time()\n",
    "        timer['training']['GDT'] = timer['training']['GDT']+t1-t0\n",
    "\n",
    "\n",
    "        ###EVALUATION\n",
    "        y_test_data =dataset_dict['y_test'].values\n",
    "        temp_X_test = dataset_dict['X_test'].values\n",
    "        temp_X_train =X_train_with_valid.values\n",
    "        temp_y_train =y_train_with_valid.values\n",
    "\n",
    "\n",
    "        t0 = time.time()\n",
    "        #GDT\n",
    "        metric = \"f1\"\n",
    "       # y_test_data = dataset_dict['y_test']\n",
    "        y_pred_GDT_train = model_dict['GDT'].predict(enforce_numpy(dataset_dict['X_train'].values))\n",
    "        y_pred_GDT_train = np.nan_to_num(y_pred_GDT_train)\n",
    "        y_pred_GDT_train = np.round(y_pred_GDT_train)\n",
    "    \n",
    "        y_pred_GDT_valid = model_dict['GDT'].predict(enforce_numpy(dataset_dict['X_valid'].values))\n",
    "        y_pred_GDT_valid = np.nan_to_num(y_pred_GDT_valid)\n",
    "        y_pred_GDT_valid = np.round(y_pred_GDT_valid)    \n",
    "    \n",
    "        y_pred_GDT_test = model_dict['GDT'].predict(enforce_numpy(temp_X_test))\n",
    "        y_pred_GDT_test = np.nan_to_num(y_pred_GDT_test)\n",
    "        y_pred_GDT_test = np.round(y_pred_GDT_test)\n",
    "        \n",
    "     \n",
    "        GDT_f1_train = f1_score(dataset_dict['y_train'], y_pred_GDT_train)\n",
    "        GDT_f1_valid = f1_score(dataset_dict['y_valid'], y_pred_GDT_valid)\n",
    "        GDT_f1_test = f1_score(dataset_dict['y_test'], y_pred_GDT_test)\n",
    "\n",
    "        t1 = time.time()\n",
    "        timer['prediction']['GDT'] = timer['prediction']['GDT']+t1-t0\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #VFDT\n",
    "        y_pred_VFDT_train = []\n",
    "        y_pred_VFDT_test = []\n",
    "        for i in range(0, len(temp_X_train)):\n",
    "            VFDT_classifier.partial_fit(np.array([temp_X_train[i]], np.float64), np.array([temp_y_train[i]], np.float64))\n",
    "        \n",
    "        for i in range(0, len(temp_X_train)):\n",
    "            y_pred_VFDT_train.append(VFDT_classifier.predict(np.array([temp_X_train[i]], np.float64))[0]) \n",
    "        for i in range(0, len(dataset_dict['X_test'])): \n",
    "            y_pred_VFDT_test.append(VFDT_classifier.predict(np.array([temp_X_test[i]], np.float64))[0])  \n",
    "        \n",
    "        VFDT_train = f1_score(temp_y_train, y_pred_VFDT_train)\n",
    "        VFDT_test = f1_score(y_test_data, y_pred_VFDT_test)\n",
    "\n",
    "\n",
    "        #CVFDT\n",
    "        y_pred_CVFDT_train = []\n",
    "        y_pred_CVFDT_test = []\n",
    "        for i in range(0, len(dataset_dict['X_train'])):\n",
    "            CVFDT_classifier.partial_fit(np.array([temp_X_train[i]], np.float64), np.array([temp_y_train[i]], np.float64))\n",
    "\n",
    "        for i in range(0, len(temp_X_train)):\n",
    "            y_pred_CVFDT_train.append(CVFDT_classifier.predict(np.array([temp_X_train[i]], np.float64))[0])    \n",
    "        for i in range(0, len(dataset_dict['X_test'])):\n",
    "            y_pred_CVFDT_test.append(CVFDT_classifier.predict(np.array([temp_X_test[i]], np.float64))[0])  \n",
    "        \n",
    "        CVFDT_train = f1_score(temp_y_train, y_pred_CVFDT_train)\n",
    "        CVFDT_test = f1_score(y_test_data,y_pred_CVFDT_test)\n",
    "                                        \n",
    "\n",
    "\n",
    "    \n",
    "        scores_GDT['train'] = np.append(scores_GDT['train'], GDT_f1_train)\n",
    "        scores_GDT['valid'] = np.append(scores_GDT['valid'], GDT_f1_valid)\n",
    "        scores_GDT['test'] = np.append(scores_GDT['test'], GDT_f1_test)\n",
    "                                        \n",
    "        scores_VFDT['train'] = np.append(scores_VFDT['train'], VFDT_train)\n",
    "        scores_VFDT['test'] = np.append(scores_VFDT['test'], VFDT_test)\n",
    "                                        \n",
    "        scores_CVFDT['train'] = np.append(scores_CVFDT['train'], CVFDT_train)\n",
    "        scores_CVFDT['test'] = np.append(scores_CVFDT['test'], CVFDT_test)\n",
    "                                        \n",
    "                                        \n",
    "       \n",
    "    results = pd.DataFrame(columns=['metric','train','validation','test'])\n",
    "    results.loc[len(results)] = ['f1',round(np.average(scores_GDT['train']),4),round(np.average(scores_GDT['valid']),4),round(np.average(scores_GDT['test']),4)]\n",
    "    results.loc[len(results)] = ['f1',round(np.average(scores_VFDT['train']),4),0,round(np.average(scores_VFDT['test']),4)]\n",
    "    results.loc[len(results)] = ['f1',round(np.average(scores_CVFDT['train']),4),0,round(np.average(scores_CVFDT['test']),4)]\n",
    "    results.to_csv('overfitting_results/'+dataset_name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8183a72-d7a4-4365-94e4-27e95007ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, validation, test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f2c67f-108e-4407-9e68-e0d5da780e59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
