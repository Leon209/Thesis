{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc898a64-7f5e-4dd4-8c36-8a84c9c23cc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation\n",
    "* This notebook is used to run all models and save the results in the 'Thesis/results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e7ba4-3a33-424d-85d6-3e0a4b1bc998",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd07edfa-1d08-45b5-8dfc-b1e139a14099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/lurny/files/Thesis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1c25207-6a37-4b41-8a97-0fe4a877110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.configs import *\n",
    "config_1 = get_config_for_dataset('rbf_m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed67d911-f7f1-4738-8bbd-1c5c20c7d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-28--06-54-39140752\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, ParameterSampler, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "#from pydl85 import DL85Classifier\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if config_1['computation']['use_gpu']:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(config_1['computation']['gpu_numbers'])\n",
    "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "    os.environ['XLA_FLAGS'] = \"--xla_gpu_cuda_data_dir=/usr/local/cuda-11.6\"\n",
    "    os.environ['TF_XLA_FLAGS'] = \"--tf_xla_enable_xla_devices --tf_xla_auto_jit=2\"    \n",
    "else:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'false' \n",
    "#os.environ['TF_XLA_FLAGS'] = \"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit\" \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from utilities.utilities_GDT import *\n",
    "from utilities.GDT_for_streams import *\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from itertools import product\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "from skmultiflow.trees import HoeffdingTreeClassifier\n",
    "from skmultiflow.trees import HoeffdingAdaptiveTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from livelossplot import PlotLosses\n",
    "import time\n",
    "\n",
    "tf.random.set_seed(config_1['computation']['random_seed'])\n",
    "np.random.seed(config_1['computation']['random_seed'])\n",
    "random.seed(config_1['computation']['random_seed'])\n",
    "\n",
    "from datetime import datetime\n",
    "timestr = datetime.utcnow().strftime('%Y-%m-%d--%H-%M-%S%f')\n",
    "print(timestr)\n",
    "os.makedirs(os.path.dirname(\"./evaluation_results/latex_tables/\" + timestr +\"/\"), exist_ok=True)\n",
    "\n",
    "filepath = './evaluation_results/depth' + str(config_1['gdt']['depth']) + '/' + timestr + '/'\n",
    "Path(filepath).mkdir(parents=True, exist_ok=True)    \n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "from skmultiflow.drift_detection import DDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6027aa6-692d-468d-a6a4-93b4f414a1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num XLA-GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71fa12-9914-4f47-a2e5-68a8751c8ce3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2eec461-b6f2-488d-9069-a548d8f91a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape (selected):  (10000, 36)\n",
      "Original Data Shape (encoded):  (10000, 36)\n",
      "Original Data Class Distribution:  3246  (true) / 6754  (false)\n",
      "(6000, 36) (6000,)\n",
      "(2000, 36) (2000,)\n",
      "(2000, 36) (2000,)\n",
      "Min Ratio:  0.322\n",
      "Min Ratio:  0.5\n",
      "Min Ratio:  0.32475\n",
      "Min Ratio:  0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d0d1c172374d33a4b843a321c1170a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "restarts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d491653d3a694c84ad3a404b3bed4363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 127\u001b[0m\n\u001b[1;32m    125\u001b[0m ddm_array \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(X_data\u001b[38;5;241m.\u001b[39mcolumns)):\n\u001b[0;32m--> 127\u001b[0m     \u001b[43mddm_array\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mappend(DDM())\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#Start Stream\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_data_stream)):\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "#'NOAA_Weather','agr_a','agr_g','airlines','electricity','hyperplane','rbf_f','rbf_m','sea_a','sea_g'\n",
    "dataset_names = ['agr_a']\n",
    "for dataset_name in dataset_names:\n",
    "    config = get_config_for_dataset(dataset_name)\n",
    "\n",
    "    VFDT_classifier = HoeffdingTreeClassifier()\n",
    "    CVFDT_classifier = HoeffdingAdaptiveTreeClassifier(split_confidence=0.0001)\n",
    "\n",
    "    config_training, metrics = prepare_training_for_streams(identifier = 'BIN:'+dataset_name, config = config)\n",
    "    #load Dataset\n",
    "    X_data, y_data, nominal_features, ordinal_features = load_dataset_for_streams(identifier = 'BIN:'+dataset_name, \n",
    "                                                                              max_total_samples = config['computation']['max_total_samples'])\n",
    "\n",
    "    model_dict = {}\n",
    "    scores_dict = {}\n",
    "    normalizer_list = []\n",
    "    plotlosses_benchmark = PlotLosses()\n",
    "    verbosity = 1\n",
    "    model_dict['GDT'] = GDT(number_of_variables = len(X_data.columns),\n",
    "                number_of_classes = len(np.unique(y_data)),#dataset_dict['number_of_classes'],\n",
    "\n",
    "                objective = config_training['gdt']['objective'],\n",
    "\n",
    "                depth = config_training['gdt']['depth'],\n",
    "\n",
    "                learning_rate_index = config_training['gdt']['learning_rate_index'],\n",
    "                learning_rate_values = config_training['gdt']['learning_rate_values'],\n",
    "                learning_rate_leaf = config_training['gdt']['learning_rate_leaf'],\n",
    "\n",
    "                optimizer = config_training['gdt']['optimizer'],\n",
    "\n",
    "                loss = 'crossentropy',\n",
    "\n",
    "                initializer_values = config_training['gdt']['initializer_values'],\n",
    "                initializer_index = config_training['gdt']['initializer_index'],\n",
    "                initializer_leaf = config_training['gdt']['initializer_leaf'],        \n",
    "\n",
    "                random_seed = config_training['computation']['random_seed'],\n",
    "                verbosity = verbosity)  \n",
    "\n",
    "\n",
    "    #Pretraing\n",
    "    if(config_training['computation']['pretrain_size']>0 and len(X_data) > config_training['computation']['pretrain_size']):\n",
    "        X_pretrain_data = X_data.iloc[:config_training['computation']['pretrain_size'],:]\n",
    "        X_data = X_data.iloc[config_training['computation']['pretrain_size']:,:]\n",
    "        y_pretrain_data = y_data.iloc[:config_training['computation']['pretrain_size']]\n",
    "        y_data = y_data.iloc[config_training['computation']['pretrain_size']:]\n",
    "\n",
    "        ((X_train, y_train),\n",
    "         (X_valid, y_valid),\n",
    "         (X_test, y_test),\n",
    "         (X_train_with_valid, y_train_with_valid),\n",
    "         normalizer_list) = preprocess_data(X_pretrain_data, \n",
    "                                           y_pretrain_data,\n",
    "                                           nominal_features,\n",
    "                                           ordinal_features,\n",
    "                                           config_training,\n",
    "                                           normalizer_list,\n",
    "                                           random_seed= 42,#random_seed,\n",
    "                                           verbosity=1)#verbosity)  \n",
    "        dataset_dict = {\n",
    "               'X_train': X_train,\n",
    "               'y_train': y_train,\n",
    "               'X_valid': X_valid,\n",
    "               'y_valid': y_valid,\n",
    "               'X_test': X_test,\n",
    "               'y_test': y_test,\n",
    "               'normalizer_list': normalizer_list\n",
    "               }\n",
    "\n",
    "\n",
    "\n",
    "        model_dict['GDT'].partial_fit(dataset_dict['X_train'],\n",
    "                  dataset_dict['y_train'],\n",
    "                  batch_size=config_training['gdt']['batch_size'], \n",
    "                  epochs=config_training['gdt']['pretrain_epochs'], \n",
    "                  restarts = 0,#config_test['gdt']['restarts'], \n",
    "                  #restart_type=config_test['gdt']['restart_type'], \n",
    "                  #early_stopping_epochs=config_training['gdt']['early_stopping_epochs'], \n",
    "                  #early_stopping_type=config_test['gdt']['early_stopping_type'],\n",
    "                  valid_data=(dataset_dict['X_valid'],dataset_dict['y_valid']))\n",
    "\n",
    "        #pretrain benchmarks\n",
    "        temp_X_train =X_train_with_valid.values\n",
    "        temp_y_train =y_train_with_valid.values\n",
    "        for i in range(0, len(dataset_dict['X_train'])):\n",
    "            VFDT_classifier.partial_fit(np.array([temp_X_train[i]], np.float64), np.array([temp_y_train[i]], np.float64)) \n",
    "            CVFDT_classifier.partial_fit(np.array([temp_X_train[i]], np.float64), np.array([temp_y_train[i]], np.float64))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #prepare Stream\n",
    "    X_data_stream = split_df_chunks(X_data, config['computation']['chunk_size'])\n",
    "    y_data_stream = split_df_chunks(y_data, config['computation']['chunk_size'])\n",
    "\n",
    "    scores_GDT = {'f1':np.array([]),\n",
    "                  'acc':np.array([]),\n",
    "                  'kappa':np.array([])\n",
    "                 }\n",
    "    scores_VFDT = {'f1':np.array([]),\n",
    "                  'acc':np.array([]),\n",
    "                  'kappa':np.array([])\n",
    "                 }\n",
    "    scores_CVFDT = {'f1':np.array([]),\n",
    "                  'acc':np.array([]),\n",
    "                  'kappa':np.array([])\n",
    "                 }\n",
    "    timer = {'training':{\n",
    "                'GDT':0.0,\n",
    "                'VFDT':0.0,\n",
    "                'CVFDT':0.0,\n",
    "             },\n",
    "             'prediction':{\n",
    "                'GDT':0.0,\n",
    "                'VFDT':0.0,\n",
    "                'CVFDT':0.0,\n",
    "             }\n",
    "            }\n",
    "    flag = False;\n",
    "    \n",
    "    ddm_array = []\n",
    "    for i in range(0,len(X_data.columns)):\n",
    "        ddm_array[i].append(DDM())\n",
    "\n",
    "    #Start Stream\n",
    "    for i in range(len(X_data_stream)):\n",
    "        normalizer_list=[]\n",
    "        ((X_train, y_train),\n",
    "         (X_valid, y_valid),\n",
    "         (X_test, y_test),\n",
    "         (X_train_with_valid, y_train_with_valid),\n",
    "         normalizer_list) = preprocess_data(X_data_stream[i], \n",
    "                                           y_data_stream[i],\n",
    "                                           nominal_features,\n",
    "                                           ordinal_features,\n",
    "                                           config_training,\n",
    "                                           normalizer_list,\n",
    "                                           random_seed= 42,#random_seed,\n",
    "                                           verbosity=1)#verbosity)  \n",
    "        dataset_dict = {\n",
    "               'X_train': X_train,\n",
    "               'y_train': y_train,\n",
    "               'X_valid': X_valid,\n",
    "               'y_valid': y_valid,\n",
    "               'X_test': X_test,\n",
    "               'y_test': y_test,\n",
    "               'normalizer_list': normalizer_list\n",
    "               }  \n",
    "\n",
    "\n",
    "        #Train GDTs\n",
    "        t0 = time.time()\n",
    "        #if(i==491):\n",
    "        #    flag = True\n",
    "        history = model_dict['GDT'].partial_fit(dataset_dict['X_train'],\n",
    "                  dataset_dict['y_train'],\n",
    "                  batch_size=config_training['gdt']['batch_size'], \n",
    "                  epochs=config_training['gdt']['epochs'], \n",
    "                  restarts = 0,\n",
    "                  drift_flag = flag,\n",
    "                  valid_data=(dataset_dict['X_valid'],dataset_dict['y_valid']))\n",
    "        flag=False\n",
    "        t1 = time.time()\n",
    "        timer['training']['GDT'] = timer['training']['GDT']+t1-t0\n",
    "\n",
    "\n",
    "        ###EVALUATION\n",
    "        y_test_data = dataset_dict['y_test'].values\n",
    "        temp_X_test = dataset_dict['X_test'].values\n",
    "        temp_X_train =X_train_with_valid.values\n",
    "        temp_y_train =y_train_with_valid.values\n",
    "\n",
    "        \n",
    "        \n",
    "        #GDT Results\n",
    "        t0 = time.time()\n",
    "        metric = \"f1\"\n",
    "        y_test_data = dataset_dict['y_test']\n",
    "        y_pred_GDT = model_dict['GDT'].predict(enforce_numpy(temp_X_test))\n",
    "        y_pred_GDT = np.nan_to_num(y_pred_GDT)\n",
    "        y_pred_GDT = np.round(y_pred_GDT)\n",
    "        GDT_f1 = f1_score(y_test_data, y_pred_GDT)\n",
    "        GDT_acc = accuracy_score(y_test_data, y_pred_GDT)\n",
    "        GDT_kappa = cohen_kappa_score(y_test_data,y_pred_GDT)\n",
    "        t1 = time.time()\n",
    "        timer['prediction']['GDT'] = timer['prediction']['GDT']+t1-t0\n",
    "\n",
    "        \n",
    "\n",
    "        #VFDT train + predict\n",
    "        t0 = time.time()\n",
    "        y_pred_VFDT = []\n",
    "        for i in range(0, len(temp_X_train)):\n",
    "            VFDT_classifier.partial_fit(np.array([temp_X_train[i]], np.float64), np.array([temp_y_train[i]], np.float64))\n",
    "            \n",
    "            #Drift Detection\n",
    "            for j in range(0,len(X_data.columns)):\n",
    "                ddm_array[j].add_element(temp_X_train[i][j])\n",
    "\n",
    "                if ddm_array[j].detected_warning_zone():\n",
    "                    print('Warning zone has been detected in data: ')\n",
    "                if ddm_array[j].detected_change():\n",
    "                    print('Change has been detected in data: ')\n",
    "            \n",
    "        t1 = time.time()\n",
    "        timer['training']['VFDT'] = timer['training']['VFDT']+t1-t0\n",
    "        t0 = time.time()\n",
    "        for i in range(0, len(dataset_dict['X_test'])):\n",
    "            y_pred_VFDT.append(VFDT_classifier.predict(np.array([temp_X_test[i]], np.float64))[0])  \n",
    "        VFDT_f1 = f1_score(y_test_data, y_pred_VFDT)\n",
    "        VFDT_acc = accuracy_score(y_test_data, y_pred_VFDT)\n",
    "        VFDT_kappa = cohen_kappa_score(y_test_data,y_pred_VFDT)\n",
    "        t1 = time.time()\n",
    "        timer['prediction']['VFDT'] = timer['prediction']['VFDT']+t1-t0\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #CVFDT train + predict\n",
    "        t0 = time.time()\n",
    "        y_pred_CVFDT = []\n",
    "        for i in range(0, len(dataset_dict['X_train'])):\n",
    "            CVFDT_classifier.partial_fit(np.array([temp_X_train[i]], np.float64), np.array([temp_y_train[i]], np.float64))\n",
    "        t1 = time.time()\n",
    "        timer['training']['CVFDT'] = timer['training']['CVFDT']+t1-t0\n",
    "        t0 = time.time()\n",
    "        for i in range(0, len(dataset_dict['X_test'])):\n",
    "            y_pred_CVFDT.append(CVFDT_classifier.predict(np.array([temp_X_test[i]], np.float64))[0])  \n",
    "        CVFDT_f1 = f1_score(y_test_data, y_pred_CVFDT)\n",
    "        CVFDT_acc = accuracy_score(y_test_data, y_pred_CVFDT)\n",
    "        CVFDT_kappa = cohen_kappa_score(y_test_data,y_pred_CVFDT)\n",
    "        t1 = time.time()\n",
    "        timer['prediction']['CVFDT'] = timer['prediction']['CVFDT']+t1-t0\n",
    "\n",
    "\n",
    "\n",
    "        plotlosses_benchmark.update({'GDT_f1': GDT_f1,'VFDT_f1': VFDT_f1, 'CVFDT_f1':CVFDT_f1})\n",
    "        plotlosses_benchmark.send() \n",
    "\n",
    "        \n",
    "\n",
    "        #Save scores\n",
    "        scores_GDT['f1'] = np.append(scores_GDT['f1'], GDT_f1)\n",
    "        scores_VFDT['f1'] = np.append(scores_VFDT['f1'], VFDT_f1)\n",
    "        scores_CVFDT['f1'] = np.append(scores_CVFDT['f1'], CVFDT_f1)\n",
    "\n",
    "        scores_GDT['acc'] = np.append(scores_GDT['acc'], GDT_acc)\n",
    "        scores_VFDT['acc'] = np.append(scores_VFDT['acc'], VFDT_acc)\n",
    "        scores_CVFDT['acc'] = np.append(scores_CVFDT['acc'], CVFDT_acc)\n",
    "\n",
    "        scores_GDT['kappa'] = np.append(scores_GDT['kappa'], GDT_kappa)\n",
    "        scores_VFDT['kappa'] = np.append(scores_VFDT['kappa'], VFDT_kappa)\n",
    "        scores_CVFDT['kappa'] = np.append(scores_CVFDT['kappa'], CVFDT_kappa)\n",
    "\n",
    "\n",
    "    #save_scores(dataset_name, scores_GDT, scores_VFDT, scores_CVFDT, VFDT_classifier, CVFDT_classifier, config, timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f2c67f-108e-4407-9e68-e0d5da780e59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
